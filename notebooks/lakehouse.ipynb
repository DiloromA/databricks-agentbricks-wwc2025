{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74a82ec-a8ca-44c3-9946-a787e93224fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Period Tracking Pipeline\n",
    "Layers: RAW (CSV in UC Volume) ➜ BRONZE (Delta batch) ➜ SILVER (clean) ➜ GOLD (modeled)\n",
    "\n",
    "Use the RUN_LAYER widget to run ALL or a single layer.\n",
    "(This notebook could be optionally run as a Job to demo orchestration capabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a135118-e47b-4e2d-91c7-2db447203340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgets / Params\n",
    "dbutils.widgets.text(\"CATALOG\", \"wwc2025\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"period_pipeline\")\n",
    "dbutils.widgets.text(\"RAW_VOLUME\", \"raw_period\")\n",
    "dbutils.widgets.dropdown(\"RUN_LAYER\", \"ALL\", [\"ALL\", \"RAW\", \"BRONZE\", \"SILVER\", \"GOLD\"])\n",
    "\n",
    "CATALOG   = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA    = dbutils.widgets.get(\"SCHEMA\")\n",
    "RAW_VOL   = dbutils.widgets.get(\"RAW_VOLUME\")\n",
    "RUN_LAYER = dbutils.widgets.get(\"RUN_LAYER\")\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {RAW_VOL}\")\n",
    "\n",
    "RAW_BASE = f\"/Volumes/{CATALOG}/{SCHEMA}/{RAW_VOL}\"\n",
    "\n",
    "print(f\"Catalog.Schema: {CATALOG}.{SCHEMA}\")\n",
    "print(f\"RAW volume    : {RAW_BASE}\")\n",
    "print(f\"RUN_LAYER     : {RUN_LAYER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb70f7fd-238f-46dd-89b9-f006d3222310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RAW: Generate CSVs with Faker (users, cycles, symptoms, mood_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893ba760-cd9e-4ff8-ba75-e2638033e793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if RUN_LAYER in (\"ALL\", \"RAW\"):\n",
    "    import numpy as np, pandas as pd, uuid\n",
    "    from datetime import date, timedelta, datetime\n",
    "\n",
    "    # Parameters\n",
    "    SEED    = 42\n",
    "    N_USERS = 300           # participants\n",
    "    MONTHS  = 18            # months of history\n",
    "\n",
    "    # Modern RNG (single source of randomness)\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    # Helpers / constants\n",
    "    def clamp(n, lo, hi): \n",
    "        return max(lo, min(hi, n))\n",
    "\n",
    "    CITIES   = [\"New York\",\"Chicago\",\"Dallas\",\"Seattle\",\"Miami\",\"Atlanta\",\"Los Angeles\",\"Boston\",\"Denver\"]\n",
    "    SEGMENTS = [\"student\",\"professional\",\"parent\",\"athlete\",\"shift_worker\"]\n",
    "    MOODS    = [\"happy\",\"stressed\",\"tired\",\"sad\",\"energized\",\"calm\"]\n",
    "    SYMPTOMS = [\"cramps\",\"bloating\",\"headache\",\"acne\",\"cravings\",\"back_pain\",\"fatigue\",\"nausea\"]\n",
    "\n",
    "    base_start = date.today() - timedelta(days=int(30*MONTHS))\n",
    "\n",
    "    # -------------------------\n",
    "    # users.csv\n",
    "    # -------------------------\n",
    "    users = []\n",
    "    for uid in range(1, N_USERS+1):\n",
    "        age = int(rng.normal(29, 7))\n",
    "        age = clamp(age, 16, 55)  # allow menopause band\n",
    "        users.append({\n",
    "            \"user_id\": uid, \n",
    "            \"age\": age, \n",
    "            \"city\": rng.choice(CITIES), \n",
    "            \"lifestyle_segment\": rng.choice(SEGMENTS)\n",
    "        })\n",
    "    users_df = pd.DataFrame(users)\n",
    "\n",
    "    # -------------------------\n",
    "    # cycles.csv\n",
    "    # -------------------------\n",
    "    cycles = []\n",
    "    NUM_LONG_BLEEDS    = max(6, N_USERS // 40)   # ~2.5%\n",
    "    NUM_SHORT_INTERVAL = max(6, N_USERS // 40)\n",
    "\n",
    "    long_bleed_users = set(rng.choice(np.arange(1, N_USERS+1), size=NUM_LONG_BLEEDS, replace=False).tolist())\n",
    "    short_int_users  = set(rng.choice(np.arange(1, N_USERS+1), size=NUM_SHORT_INTERVAL, replace=False).tolist())\n",
    "\n",
    "    def rand_date(start: date, end: date) -> date:\n",
    "        delta = (end - start).days\n",
    "        return start + timedelta(days=int(rng.integers(0, max(delta,1))))\n",
    "\n",
    "    for u in users:\n",
    "        uid = u[\"user_id\"]\n",
    "        base_cycle_mu = clamp(int(rng.normal(28, 3)), 23, 35)\n",
    "        cycle_mu      = clamp(base_cycle_mu + int(rng.normal(0, 2)), 22, 36)\n",
    "\n",
    "        start_d = rand_date(base_start, date.today() - timedelta(days=30))\n",
    "        prev_end = None\n",
    "\n",
    "        while start_d < date.today():\n",
    "            # period length\n",
    "            period_len = int(rng.normal(5, 1.5))\n",
    "            period_len = clamp(period_len, 2, 10)\n",
    "            # occasional long bleed (>=20d)\n",
    "            if (uid in long_bleed_users and rng.random() < 0.25) or (rng.random() < 0.02):\n",
    "                period_len = max(period_len, 20)\n",
    "\n",
    "            # expected start-to-start\n",
    "            cycle_len = int(rng.normal(cycle_mu, 2.0))\n",
    "            cycle_len = clamp(cycle_len, 20, 45)\n",
    "\n",
    "            # occasional short interval (<15d gap)\n",
    "            interval_since_prev_end = (start_d - prev_end).days if prev_end else None\n",
    "            if prev_end and ((uid in short_int_users and rng.random() < 0.25) or (rng.random() < 0.02)):\n",
    "                start_d = prev_end + timedelta(days=int(rng.integers(5, 14)))\n",
    "                interval_since_prev_end = (start_d - prev_end).days\n",
    "\n",
    "            end_d = start_d + timedelta(days=period_len - 1)\n",
    "\n",
    "            cycles.append({\n",
    "                \"user_id\": uid,\n",
    "                \"cycle_start_date\": start_d.isoformat(),\n",
    "                \"cycle_end_date\": end_d.isoformat(),\n",
    "                \"period_length_days\": period_len,\n",
    "                \"cycle_length_days\": cycle_len,\n",
    "                # keep raw interval (nullable) — will be recomputed in SILVER\n",
    "                \"interval_since_prev_end_days\": interval_since_prev_end\n",
    "            })\n",
    "\n",
    "            # next start\n",
    "            jitter = int(rng.normal(0, 2))\n",
    "            prev_end = end_d\n",
    "            start_d = start_d + timedelta(days=cycle_len + jitter)\n",
    "\n",
    "    cycles_df = pd.DataFrame(cycles)\n",
    "\n",
    "    # -------------------------\n",
    "    # symptoms.csv\n",
    "    # -------------------------\n",
    "    symptoms_rows = []\n",
    "    for _, row in cycles_df.iterrows():\n",
    "        uid = int(row.user_id)\n",
    "        s   = pd.to_datetime(row.cycle_start_date).date()\n",
    "        e   = pd.to_datetime(row.cycle_end_date).date()\n",
    "        days = (e - s).days + 1\n",
    "        for d in range(days):\n",
    "            if rng.random() < 0.6:\n",
    "                symptoms_rows.append({\n",
    "                    \"user_id\": uid,\n",
    "                    \"date\": (s + timedelta(days=d)).isoformat(),\n",
    "                    \"symptom\": rng.choice(SYMPTOMS),\n",
    "                    \"severity\": int(clamp(int(rng.normal(3, 1.1)), 1, 5))\n",
    "                })\n",
    "    symptoms_df = pd.DataFrame(symptoms_rows)\n",
    "\n",
    "    # -------------------------\n",
    "    # mood_logs.csv\n",
    "    # -------------------------\n",
    "    mood_rows = []\n",
    "    for uid in users_df[\"user_id\"].tolist():\n",
    "        cursor = base_start\n",
    "        while cursor <= date.today():\n",
    "            if rng.random() < 0.5:\n",
    "                mood_rows.append({\n",
    "                    \"user_id\": int(uid),\n",
    "                    \"date\": cursor.isoformat(),\n",
    "                    \"mood\": rng.choice(MOODS),\n",
    "                    \"notes\": rng.choice([\"\", \"\", \"felt low energy\", \"great workout\", \"busy day\", \"travel\", \"family event\"])\n",
    "                })\n",
    "            cursor += timedelta(days=3)\n",
    "    mood_df = pd.DataFrame(mood_rows)\n",
    "\n",
    "    # user_prefs.csv\n",
    "    def pick_bool(p): \n",
    "        return bool(rng.choice([1,0], p=[p, 1-p]))\n",
    "\n",
    "    prefs = []\n",
    "    for r in users_df.itertuples(index=False):\n",
    "        age = int(getattr(r, \"age\", rng.integers(16, 52)))\n",
    "        prefs.append({\n",
    "            \"user_id\": int(r.user_id),\n",
    "            \"vegan\": pick_bool(0.12),\n",
    "            \"dairy_free\": pick_bool(0.18),\n",
    "            \"caffeine_free\": pick_bool(0.20),\n",
    "            \"sensitive_skin\": pick_bool(0.25),\n",
    "            \"nut_allergy\": pick_bool(0.04),\n",
    "            \"gluten_free\": pick_bool(0.07),\n",
    "            \"sexually_active\": pick_bool(0.55 if age>=18 else 0.10),\n",
    "            \"temperament\": rng.choice([\"balanced\",\"anxious\",\"low-energy\",\"irritable\"], p=[0.45,0.20,0.20,0.15])\n",
    "        })\n",
    "    user_prefs_df = pd.DataFrame(prefs)\n",
    "\n",
    "    # --- meal_catalog.csv (tags as ';' delimited for RAW; parsed in SILVER) ---\n",
    "    meal_rows = [\n",
    "        (\"Spinach Lentil Bowl\",\"lunch\",\"high-iron;vegan;gluten-free;anti-inflammatory\"),\n",
    "        (\"Tofu Stir-Fry\",\"dinner\",\"vegan;high-protein;anti-inflammatory\"),\n",
    "        (\"Greek Yogurt + Berries\",\"breakfast\",\"dairy;low-sugar;anti-inflammatory\"),\n",
    "        (\"Oatmeal + Pumpkin Seeds\",\"breakfast\",\"high-iron;vegan\"),\n",
    "        (\"Salmon + Quinoa\",\"dinner\",\"omega3;gluten-free;anti-inflammatory\"),\n",
    "        (\"Chickpea Pasta + Pesto\",\"dinner\",\"vegan;high-protein\"),\n",
    "        (\"Avocado Toast + Seeds\",\"breakfast\",\"vegan;low-sugar\"),\n",
    "        (\"Turkey & Greens Wrap\",\"lunch\",\"high-protein;low-sugar\"),\n",
    "    ]\n",
    "    meals_df = pd.DataFrame([{\n",
    "        \"meal_id\": str(uuid.uuid4()), \"name\": n, \"meal_type\": mt, \"tags\": tags\n",
    "    } for (n,mt,tags) in meal_rows])\n",
    "\n",
    "    # --- supplement_catalog.csv ---\n",
    "    supp_rows = [\n",
    "        (\"Magnesium Glycinate 200mg\",\"sleep;cramps;calm;caffeine-free\"),\n",
    "        (\"Omega-3 Softgels\",\"omega3;anti-inflammatory\"),\n",
    "        (\"Electrolyte Powder (no sugar)\",\"electrolyte;caffeine-free\"),\n",
    "        (\"Iron Complex (gentle)\",\"iron;with-food\"),\n",
    "        (\"Probiotic Blend\",\"gut;daily\"),\n",
    "    ]\n",
    "    supp_df = pd.DataFrame([{\n",
    "        \"supplement_id\": str(uuid.uuid4()), \"name\": n, \"tags\": tags\n",
    "    } for (n,tags) in supp_rows])\n",
    "\n",
    "    # --- workout_catalog.csv ---\n",
    "    wkt_rows = [\n",
    "        (\"Rest day\",\"none\",\"recovery\",\"at-home;no-equipment;low-impact\"),\n",
    "        (\"Gentle yoga 20m\",\"low\",\"mobility\",\"at-home;no-equipment;low-impact\"),\n",
    "        (\"Walk 30m\",\"low\",\"cardio\",\"at-home\"),\n",
    "        (\"Strength circuit 30m\",\"moderate\",\"strength\",\"at-home\"),\n",
    "        (\"Low-impact cardio 25m\",\"low\",\"cardio\",\"low-impact\"),\n",
    "    ]\n",
    "    wkt_df = pd.DataFrame([{\n",
    "        \"workout_id\": str(uuid.uuid4()), \"name\": n, \"intensity\": it, \"focus\": f, \"tags\": tags\n",
    "    } for (n,it,f,tags) in wkt_rows])\n",
    "\n",
    "    # --- hygiene_catalog.csv ---\n",
    "    hyg_rows = [\n",
    "        (\"Ultra-thin pads\",\"pad\",\"sensitive-skin;fragrance-free\"),\n",
    "        (\"Organic cotton tampons\",\"tampon\",\"sustainable\"),\n",
    "        (\"Period underwear\",\"period-underwear\",\"sustainable;sensitive-skin\"),\n",
    "        (\"pH-balanced wash\",\"wash\",\"fragrance-free;sensitive-skin\"),\n",
    "        (\"Soothing wipes\",\"wipe\",\"fragrance-free\"),\n",
    "        (\"Heat Patch (12h)\",\"heat-patch\",\"heat-therapy\"),\n",
    "    ]\n",
    "    hyg_df = pd.DataFrame([{\n",
    "        \"hygiene_id\": str(uuid.uuid4()), \"name\": n, \"type\": t, \"tags\": tags\n",
    "    } for (n,t,tags) in hyg_rows])\n",
    "\n",
    "    # --- wellness_phase_rules.csv (phase x life_stage policy; arrays as ';' strings) ---\n",
    "    rules_df = pd.DataFrame([\n",
    "        (\"menstrual\",\"teen\",\"iron-rich;heat-therapy;sensitive-skin;caffeine-free\",\"high-caffeine\",\"high-iron;anti-inflammatory;low-sugar\",\"\",\"low\",\"pad;period-underwear;heat-patch\",\"magnesium;electrolyte\",\"\"),\n",
    "        (\"menstrual\",\"reproductive\",\"iron-rich;heat-therapy;sensitive-skin;caffeine-free\",\"high-caffeine\",\"high-iron;anti-inflammatory;low-sugar\",\"\",\"low\",\"pad;period-underwear;heat-patch\",\"magnesium;electrolyte\",\"\"),\n",
    "        (\"menstrual\",\"perimenopause\",\"iron-rich;heat-therapy;sensitive-skin;caffeine-free\",\"high-caffeine\",\"anti-inflammatory;low-sugar\",\"\",\"low\",\"pad;period-underwear;heat-patch\",\"magnesium;electrolyte\",\"\"),\n",
    "        (\"menstrual\",\"menopause\",\"heat-therapy;sensitive-skin;caffeine-free\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"low\",\"heat-patch;wash;wipe\",\"magnesium\",\"\"),\n",
    "        (\"follicular\",\"teen\",\"high-protein\",\"\",\"high-protein;anti-inflammatory\",\"\",\"moderate\",\"wash;wipe\",\"electrolyte\",\"\"),\n",
    "        (\"follicular\",\"reproductive\",\"high-protein;omega3\",\"\",\"high-protein;anti-inflammatory\",\"\",\"moderate\",\"wash;wipe\",\"omega3;electrolyte\",\"\"),\n",
    "        (\"follicular\",\"perimenopause\",\"omega3\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"moderate\",\"wash;wipe\",\"omega3\",\"\"),\n",
    "        (\"follicular\",\"menopause\",\"omega3;caffeine-free\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"moderate\",\"wash;wipe\",\"omega3\",\"\"),\n",
    "        (\"ovulation\",\"reproductive\",\"electrolyte;caffeine-free\",\"\",\"light;anti-inflammatory\",\"\",\"moderate\",\"wash;wipe\",\"electrolyte\",\"\"),\n",
    "        (\"luteal\",\"teen\",\"magnesium;caffeine-free\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"low\",\"heat-patch;wash;wipe\",\"magnesium\",\"\"),\n",
    "        (\"luteal\",\"reproductive\",\"magnesium;caffeine-free\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"low\",\"heat-patch;wash;wipe\",\"magnesium\",\"\"),\n",
    "        (\"luteal\",\"perimenopause\",\"magnesium;caffeine-free\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"low\",\"heat-patch;wash;wipe\",\"magnesium\",\"\"),\n",
    "        (\"luteal\",\"menopause\",\"caffeine-free\",\"\",\"anti-inflammatory;low-sugar\",\"\",\"low\",\"heat-patch;wash;wipe\",\"magnesium\",\"\"),\n",
    "    ], columns=[\n",
    "        \"phase\",\"life_stage\",\"prefer_product_tags\",\"avoid_product_tags\",\n",
    "        \"prefer_meal_tags\",\"avoid_meal_tags\",\"workout_intensity\",\"hygiene_types\",\n",
    "        \"prefer_supp_tags\",\"avoid_supp_tags\"\n",
    "    ])\n",
    "\n",
    "    # -------------------------\n",
    "    # Write RAW CSVs to UC Volume\n",
    "    # -------------------------\n",
    "    raw_tables = {\n",
    "        # original\n",
    "        \"users\": users_df,\n",
    "        \"cycles\": cycles_df,\n",
    "        \"symptoms\": symptoms_df,\n",
    "        \"mood_logs\": mood_df,\n",
    "        \"user_prefs\": user_prefs_df,\n",
    "        \"meal_catalog\": meals_df,\n",
    "        \"supplement_catalog\": supp_df,\n",
    "        \"workout_catalog\": wkt_df,\n",
    "        \"hygiene_catalog\": hyg_df,\n",
    "        \"wellness_phase_rules\": rules_df\n",
    "    }\n",
    "\n",
    "    for name, df in raw_tables.items():\n",
    "        out_dir = f\"{RAW_BASE}/{name}\"\n",
    "        dbutils.fs.rm(out_dir, recurse=True)\n",
    "        spark.createDataFrame(df).coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(out_dir)\n",
    "        print(f\"Wrote RAW CSV: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89ed6ec-abf9-4a55-9eb1-4bd13515f944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BRONZE: Batch load RAW CSV ➜ Delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef2b7d8d-1c5a-414b-af4f-4151e366c345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def load_csv_once(raw_path: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Read CSVs from UC Volumes (Data Source V2) so `_metadata.file_path` is available.\n",
    "    Add `_ingest_ts` and write to a BRONZE Delta table in UC.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark.read\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", True)\n",
    "             .option(\"inferSchema\", True)\n",
    "             .load(raw_path)\n",
    "             .selectExpr(\"*\", \"_metadata.file_path as _ingest_file\")\n",
    "             .withColumn(\"_ingest_ts\", current_timestamp())\n",
    "    )\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.bronze_{table_name}\")\n",
    "    print(f\"Created table {CATALOG}.{SCHEMA}.bronze_{table_name}\")\n",
    "\n",
    "if RUN_LAYER in (\"ALL\", \"BRONZE\"):\n",
    "    bronze_tables = [\n",
    "        \"users\", \"cycles\", \"symptoms\", \"mood_logs\",\n",
    "        \"user_prefs\", \"meal_catalog\", \"supplement_catalog\",\n",
    "        \"workout_catalog\", \"hygiene_catalog\", \"wellness_phase_rules\"\n",
    "    ]\n",
    "\n",
    "    for name in bronze_tables:\n",
    "        load_csv_once(f\"{RAW_BASE}/{name}\", name)\n",
    "\n",
    "    # Row counts for sanity check\n",
    "    for name in bronze_tables:\n",
    "        cnt = spark.table(f\"{CATALOG}.{SCHEMA}.bronze_{name}\").count()\n",
    "        print(f\"bronze_{name}: {cnt:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a76cc0-7645-43c5-920a-955ff639bd78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SILVER: Clean / Type / Dedupe / Derive interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6acb8b-5921-4ef8-87ee-37a14f81db35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- SILVER (combined original + wellness datasets) ----------\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "\n",
    "def _split_tags_lower(colname: str):\n",
    "    # Split on ';', trim and lowercase, and drop empties\n",
    "    arr = F.split(F.coalesce(F.col(colname), F.lit(\"\")), F.lit(\";\"))\n",
    "    arr = F.transform(arr, lambda x: F.lower(F.trim(x)))\n",
    "    return F.filter(arr, lambda x: x != F.lit(\"\"))\n",
    "\n",
    "# ===== Original =====\n",
    "\n",
    "def silver_users():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_users\")\n",
    "          .select(\n",
    "              F.col(\"user_id\").cast(\"int\").alias(\"user_id\"),\n",
    "              F.col(\"age\").cast(\"int\").alias(\"age\"),\n",
    "              F.initcap(F.col(\"city\")).alias(\"city\"),\n",
    "              F.col(\"lifestyle_segment\").alias(\"segment\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          )\n",
    "          .dropDuplicates([\"user_id\"])\n",
    "         )\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_users\")\n",
    "\n",
    "def silver_cycles():\n",
    "    df = spark.table(f\"{CATALOG}.{SCHEMA}.bronze_cycles\")\n",
    "    df = (df\n",
    "          .withColumn(\"user_id\", F.col(\"user_id\").cast(\"int\"))\n",
    "          .withColumn(\"cycle_start_date\", F.to_date(\"cycle_start_date\"))\n",
    "          .withColumn(\"cycle_end_date\",   F.to_date(\"cycle_end_date\"))\n",
    "          .withColumn(\"period_length_days\", F.col(\"period_length_days\").cast(\"int\"))\n",
    "          .withColumn(\"cycle_length_days\",  F.col(\"cycle_length_days\").cast(\"int\"))\n",
    "          .withColumn(\"_ingest_file\", F.col(\"_ingest_file\"))\n",
    "          .withColumn(\"_ingest_ts\",   F.col(\"_ingest_ts\"))\n",
    "          .filter(F.col(\"cycle_start_date\").isNotNull() & F.col(\"cycle_end_date\").isNotNull())\n",
    "          .filter(F.col(\"period_length_days\").between(1, 60))\n",
    "         )\n",
    "\n",
    "    # Recompute interval_since_prev_end_days reliably (start - previous end)\n",
    "    w = W.partitionBy(\"user_id\").orderBy(F.col(\"cycle_start_date\").asc())\n",
    "    df = (df\n",
    "          .withColumn(\"prev_end\", F.lag(\"cycle_end_date\").over(w))\n",
    "          .withColumn(\"interval_since_prev_end_days\",\n",
    "                      F.when(F.col(\"prev_end\").isNull(), F.lit(None).cast(\"int\"))\n",
    "                       .otherwise(F.datediff(F.col(\"cycle_start_date\"), F.col(\"prev_end\"))))\n",
    "          .drop(\"prev_end\")\n",
    "         ) \\\n",
    "         .dropDuplicates([\"user_id\",\"cycle_start_date\",\"cycle_end_date\"])\n",
    "\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_cycles\")\n",
    "\n",
    "def silver_symptoms():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_symptoms\")\n",
    "          .withColumn(\"user_id\", F.col(\"user_id\").cast(\"int\"))\n",
    "          .withColumn(\"date\", F.to_date(\"date\"))\n",
    "          .withColumn(\"symptom\", F.lower(F.col(\"symptom\")))\n",
    "          .withColumn(\"severity\", F.col(\"severity\").cast(\"int\"))\n",
    "          .filter(F.col(\"date\").isNotNull())\n",
    "         )\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_symptoms\")\n",
    "\n",
    "def silver_mood_logs():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_mood_logs\")\n",
    "          .withColumn(\"user_id\", F.col(\"user_id\").cast(\"int\"))\n",
    "          .withColumn(\"date\", F.to_date(\"date\"))\n",
    "          .withColumn(\"mood\", F.lower(F.col(\"mood\")))\n",
    "          .withColumn(\"notes\", F.col(\"notes\"))\n",
    "          .filter(F.col(\"date\").isNotNull())\n",
    "         )\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_mood_logs\")\n",
    "\n",
    "# ===== New wellness =====\n",
    "\n",
    "def silver_user_prefs():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_user_prefs\")\n",
    "          .select(\n",
    "              F.col(\"user_id\").cast(\"int\").alias(\"user_id\"),\n",
    "              F.col(\"vegan\").cast(\"boolean\"),\n",
    "              F.col(\"dairy_free\").cast(\"boolean\"),\n",
    "              F.col(\"caffeine_free\").cast(\"boolean\"),\n",
    "              F.col(\"sensitive_skin\").cast(\"boolean\"),\n",
    "              F.col(\"nut_allergy\").cast(\"boolean\"),\n",
    "              F.col(\"gluten_free\").cast(\"boolean\"),\n",
    "              F.col(\"sexually_active\").cast(\"boolean\"),\n",
    "              F.lower(F.col(\"temperament\")).alias(\"temperament\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          )\n",
    "          .dropDuplicates([\"user_id\"])\n",
    "         )\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_user_prefs\")\n",
    "\n",
    "def silver_meal_catalog():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_meal_catalog\")\n",
    "          .select(\n",
    "              F.col(\"meal_id\").cast(\"string\").alias(\"meal_id\"),\n",
    "              F.col(\"name\"),\n",
    "              F.col(\"meal_type\"),\n",
    "              _split_tags_lower(\"tags\").alias(\"tags\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          ))\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_meal_catalog\")\n",
    "\n",
    "def silver_supplement_catalog():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_supplement_catalog\")\n",
    "          .select(\n",
    "              F.col(\"supplement_id\").cast(\"string\").alias(\"supplement_id\"),\n",
    "              F.col(\"name\"),\n",
    "              _split_tags_lower(\"tags\").alias(\"tags\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          ))\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_supplement_catalog\")\n",
    "\n",
    "def silver_workout_catalog():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_workout_catalog\")\n",
    "          .select(\n",
    "              F.col(\"workout_id\").cast(\"string\").alias(\"workout_id\"),\n",
    "              F.col(\"name\"),\n",
    "              F.lower(F.col(\"intensity\")).alias(\"intensity\"),\n",
    "              F.lower(F.col(\"focus\")).alias(\"focus\"),\n",
    "              _split_tags_lower(\"tags\").alias(\"tags\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          ))\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_workout_catalog\")\n",
    "\n",
    "def silver_hygiene_catalog():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_hygiene_catalog\")\n",
    "          .select(\n",
    "              F.col(\"hygiene_id\").cast(\"string\").alias(\"hygiene_id\"),\n",
    "              F.col(\"name\"),\n",
    "              F.lower(F.col(\"type\")).alias(\"type\"),\n",
    "              _split_tags_lower(\"tags\").alias(\"tags\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          ))\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_hygiene_catalog\")\n",
    "\n",
    "def silver_wellness_phase_rules():\n",
    "    df = (spark.table(f\"{CATALOG}.{SCHEMA}.bronze_wellness_phase_rules\")\n",
    "          .select(\n",
    "              F.lower(F.col(\"phase\")).alias(\"phase\"),\n",
    "              F.lower(F.col(\"life_stage\")).alias(\"life_stage\"),\n",
    "              _split_tags_lower(\"prefer_product_tags\").alias(\"prefer_product_tags\"),\n",
    "              _split_tags_lower(\"avoid_product_tags\").alias(\"avoid_product_tags\"),\n",
    "              _split_tags_lower(\"prefer_meal_tags\").alias(\"prefer_meal_tags\"),\n",
    "              _split_tags_lower(\"avoid_meal_tags\").alias(\"avoid_meal_tags\"),\n",
    "              F.lower(F.col(\"workout_intensity\")).alias(\"workout_intensity\"),\n",
    "              _split_tags_lower(\"hygiene_types\").alias(\"hygiene_types\"),\n",
    "              _split_tags_lower(\"prefer_supp_tags\").alias(\"prefer_supp_tags\"),\n",
    "              _split_tags_lower(\"avoid_supp_tags\").alias(\"avoid_supp_tags\"),\n",
    "              F.col(\"_ingest_file\"), F.col(\"_ingest_ts\")\n",
    "          ))\n",
    "    df.write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_wellness_phase_rules\")\n",
    "\n",
    "# Execute all SILVER transforms in one go\n",
    "if RUN_LAYER in (\"ALL\", \"SILVER\"):\n",
    "    silver_users()\n",
    "    silver_cycles()\n",
    "    silver_symptoms()\n",
    "    silver_mood_logs()\n",
    "\n",
    "    silver_user_prefs()\n",
    "    silver_meal_catalog()\n",
    "    silver_supplement_catalog()\n",
    "    silver_workout_catalog()\n",
    "    silver_hygiene_catalog()\n",
    "    silver_wellness_phase_rules()\n",
    "\n",
    "    for t in [\n",
    "        \"users\",\"cycles\",\"symptoms\",\"mood_logs\",\n",
    "        \"user_prefs\",\"meal_catalog\",\"supplement_catalog\",\n",
    "        \"workout_catalog\",\"hygiene_catalog\",\"wellness_phase_rules\"\n",
    "    ]:\n",
    "        print(f\"silver_{t}: {spark.table(f'{CATALOG}.{SCHEMA}.silver_{t}').count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afaaaff-df5f-4454-8a9c-f6f76ecdc89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## GOLD: Modeled tables & view\n",
    "- `gold_cycles` with irregular flags\n",
    "- `gold_user_cycle_metrics` per-user rollups\n",
    "- `vw_cycle_irregularities` helper for demos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd01d383-fc87-4440-baed-a3f6ef3949f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def gold_cycles():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {CATALOG}.{SCHEMA}.gold_cycles AS\n",
    "        SELECT\n",
    "          user_id,\n",
    "          cycle_start_date,\n",
    "          cycle_end_date,\n",
    "          period_length_days,\n",
    "          cycle_length_days,\n",
    "          interval_since_prev_end_days,\n",
    "          (period_length_days >= 11)                                     AS is_abnormally_long_period,\n",
    "          (interval_since_prev_end_days IS NOT NULL AND interval_since_prev_end_days < 15) AS is_short_interval\n",
    "        FROM {CATALOG}.{SCHEMA}.silver_cycles\n",
    "    \"\"\")\n",
    "    print(\"gold_cycles                :\", spark.table(f\"{CATALOG}.{SCHEMA}.gold_cycles\").count())\n",
    "\n",
    "def gold_user_cycle_metrics():\n",
    "    # per-user averages & counts (simple group-by)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {CATALOG}.{SCHEMA}.gold_user_cycle_metrics AS\n",
    "        SELECT\n",
    "          user_id,\n",
    "          ROUND(AVG(cycle_length_days), 1)  AS avg_cycle_length_days,\n",
    "          ROUND(AVG(period_length_days), 1) AS avg_period_length_days,\n",
    "          COUNT(*)                           AS cycles_count\n",
    "        FROM {CATALOG}.{SCHEMA}.gold_cycles\n",
    "        GROUP BY user_id\n",
    "    \"\"\")\n",
    "    print(\"gold_user_cycle_metrics    :\", spark.table(f\"{CATALOG}.{SCHEMA}.gold_user_cycle_metrics\").count())\n",
    "\n",
    "def gold_irregularities_view():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.vw_cycle_irregularities AS\n",
    "        SELECT\n",
    "          c.user_id,\n",
    "          c.cycle_start_date,\n",
    "          c.cycle_end_date,\n",
    "          c.period_length_days,\n",
    "          c.interval_since_prev_end_days,\n",
    "          c.is_abnormally_long_period,\n",
    "          c.is_short_interval\n",
    "        FROM {CATALOG}.{SCHEMA}.gold_cycles c\n",
    "        WHERE c.is_abnormally_long_period = TRUE OR c.is_short_interval = TRUE\n",
    "        ORDER BY user_id, cycle_start_date\n",
    "    \"\"\")\n",
    "    print(\"vw_cycle_irregularities    : ready\")\n",
    "\n",
    "def gold_user_lifestage_view():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.gold_user_lifestage AS\n",
    "        SELECT\n",
    "          u.user_id,\n",
    "          u.age,\n",
    "          CASE\n",
    "            WHEN u.age < 18 THEN 'teen'\n",
    "            WHEN u.age BETWEEN 18 AND 39 THEN 'reproductive'\n",
    "            WHEN u.age BETWEEN 40 AND 47 THEN 'perimenopause'\n",
    "            ELSE 'menopause'\n",
    "          END AS life_stage\n",
    "        FROM {CATALOG}.{SCHEMA}.silver_users u\n",
    "    \"\"\")\n",
    "    print(\"gold_user_lifestage        : ready\")\n",
    "\n",
    "def gold_user_current_phase_view():\n",
    "    # simple heuristic using last start and averages\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.gold_user_current_phase AS\n",
    "        WITH last_cycle AS (\n",
    "          SELECT user_id, MAX(cycle_start_date) AS last_start\n",
    "          FROM {CATALOG}.{SCHEMA}.gold_cycles\n",
    "          GROUP BY user_id\n",
    "        ),\n",
    "        avg AS (\n",
    "          SELECT\n",
    "            user_id,\n",
    "            CAST(ROUND(avg_period_length_days) AS INT) AS avg_period_len,\n",
    "            CAST(ROUND(avg_cycle_length_days)  AS INT) AS avg_cycle_len\n",
    "          FROM {CATALOG}.{SCHEMA}.gold_user_cycle_metrics\n",
    "        )\n",
    "        SELECT\n",
    "          a.user_id,\n",
    "          last_start,\n",
    "          avg_period_len,\n",
    "          avg_cycle_len,\n",
    "          CASE\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_start) < COALESCE(avg_period_len,5) THEN 'menstrual'\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_start) < COALESCE(avg_period_len,5) + 7 THEN 'follicular'\n",
    "            WHEN DATEDIFF(CURRENT_DATE(), last_start) < COALESCE(avg_period_len,5) + 12 THEN 'ovulation'\n",
    "            ELSE 'luteal'\n",
    "          END AS phase\n",
    "        FROM last_cycle l\n",
    "        JOIN avg a ON a.user_id = l.user_id\n",
    "    \"\"\")\n",
    "    print(\"gold_user_current_phase    : ready\")\n",
    "\n",
    "def gold_user_wellness_profile_view():\n",
    "    # top symptoms (90d) + top moods (90d)\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.gold_user_wellness_profile AS\n",
    "        WITH recent_sym AS (\n",
    "          SELECT user_id, symptom, COUNT(*) AS cnt\n",
    "          FROM {CATALOG}.{SCHEMA}.silver_symptoms\n",
    "          WHERE date >= DATEADD(day, -90, CURRENT_DATE())\n",
    "          GROUP BY user_id, symptom\n",
    "        ),\n",
    "        ranked_sym AS (\n",
    "          SELECT\n",
    "            user_id, symptom, cnt,\n",
    "            ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY cnt DESC, symptom ASC) AS rk\n",
    "          FROM recent_sym\n",
    "        ),\n",
    "        top_sym AS (\n",
    "          SELECT user_id, COLLECT_LIST(symptom) AS top_symptoms_90d\n",
    "          FROM ranked_sym\n",
    "          WHERE rk <= 3\n",
    "          GROUP BY user_id\n",
    "        ),\n",
    "\n",
    "        recent_mood AS (\n",
    "          SELECT user_id, mood, COUNT(*) AS cnt\n",
    "          FROM {CATALOG}.{SCHEMA}.silver_mood_logs\n",
    "          WHERE date >= DATEADD(day, -90, CURRENT_DATE())\n",
    "          GROUP BY user_id, mood\n",
    "        ),\n",
    "        ranked_mood AS (\n",
    "          SELECT\n",
    "            user_id, mood, cnt,\n",
    "            ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY cnt DESC, mood ASC) AS rk\n",
    "          FROM recent_mood\n",
    "        ),\n",
    "        top_mood AS (\n",
    "          SELECT user_id, COLLECT_LIST(mood) AS top_moods_90d\n",
    "          FROM ranked_mood\n",
    "          WHERE rk <= 2\n",
    "          GROUP BY user_id\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "          u.user_id,\n",
    "          COALESCE(tm.top_moods_90d, ARRAY())    AS top_moods_90d,\n",
    "          COALESCE(ts.top_symptoms_90d, ARRAY()) AS top_symptoms_90d\n",
    "        FROM {CATALOG}.{SCHEMA}.silver_users u\n",
    "        LEFT JOIN top_mood tm ON tm.user_id = u.user_id\n",
    "        LEFT JOIN top_sym ts  ON ts.user_id = u.user_id\n",
    "    \"\"\")\n",
    "    print(\"gold_user_wellness_profile : ready\")\n",
    "\n",
    "def gold_user_wellness_context_view():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.gold_user_wellness_context AS\n",
    "        SELECT\n",
    "          u.user_id,\n",
    "          ls.life_stage,\n",
    "          cp.phase,\n",
    "          p.vegan, p.dairy_free, p.caffeine_free, p.sensitive_skin, p.nut_allergy, p.gluten_free,\n",
    "          p.sexually_active, p.temperament,\n",
    "          wp.top_moods_90d, wp.top_symptoms_90d\n",
    "        FROM {CATALOG}.{SCHEMA}.silver_user_prefs p\n",
    "        JOIN {CATALOG}.{SCHEMA}.silver_users u                    ON u.user_id = p.user_id\n",
    "        LEFT JOIN {CATALOG}.{SCHEMA}.gold_user_lifestage ls       ON ls.user_id = u.user_id\n",
    "        LEFT JOIN {CATALOG}.{SCHEMA}.gold_user_current_phase cp   ON cp.user_id = u.user_id\n",
    "        LEFT JOIN {CATALOG}.{SCHEMA}.gold_user_wellness_profile wp ON wp.user_id = u.user_id\n",
    "    \"\"\")\n",
    "    print(\"gold_user_wellness_context : ready\")\n",
    "\n",
    "# ===== Optional: UC table function to power the wellness agent =====\n",
    "def create_recommend_wellness_bundle_function():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.recommend_wellness_bundle(p_user_id INT, p_items INT)\n",
    "        RETURNS TABLE (\n",
    "          user_id INT,\n",
    "          phase STRING,\n",
    "          life_stage STRING,\n",
    "          section STRING,   -- \"meal\"|\"supplement\"|\"workout\"|\"hygiene\"\n",
    "          id STRING,\n",
    "          name STRING,\n",
    "          meta STRING       -- JSON blob (tags, etc.)\n",
    "        )\n",
    "        RETURN\n",
    "        WITH\n",
    "        ctx AS (\n",
    "          SELECT * FROM {CATALOG}.{SCHEMA}.gold_user_wellness_context WHERE user_id = p_user_id\n",
    "        ),\n",
    "        rules AS (\n",
    "          SELECT * FROM {CATALOG}.{SCHEMA}.silver_wellness_phase_rules\n",
    "        ),\n",
    "\n",
    "        -- meals: rank matches then keep top p_items\n",
    "        m_ranked AS (\n",
    "          SELECT\n",
    "            c.user_id, c.phase, c.life_stage,\n",
    "            'meal' AS section,\n",
    "            mc.meal_id AS id, mc.name,\n",
    "            TO_JSON(NAMED_STRUCT('tags', mc.tags)) AS meta,\n",
    "            ROW_NUMBER() OVER (\n",
    "              ORDER BY SIZE(ARRAY_INTERSECT(mc.tags, r.prefer_meal_tags)) DESC, mc.name ASC\n",
    "            ) AS rn\n",
    "          FROM ctx c\n",
    "          JOIN rules r ON r.phase = c.phase AND r.life_stage = c.life_stage\n",
    "          JOIN {CATALOG}.{SCHEMA}.silver_meal_catalog mc\n",
    "          WHERE SIZE(ARRAY_INTERSECT(mc.tags, r.prefer_meal_tags)) > 0\n",
    "            AND ( (c.vegan = TRUE        AND ARRAY_CONTAINS(mc.tags, 'vegan'))         OR c.vegan        = FALSE )\n",
    "            AND ( (c.gluten_free = TRUE  AND ARRAY_CONTAINS(mc.tags, 'gluten-free'))   OR c.gluten_free  = FALSE )\n",
    "            AND ( (c.dairy_free  = TRUE  AND NOT ARRAY_CONTAINS(mc.tags, 'dairy'))     OR c.dairy_free   = FALSE )\n",
    "        ),\n",
    "        m AS (\n",
    "          SELECT user_id, phase, life_stage, section, id, name, meta\n",
    "          FROM m_ranked\n",
    "          WHERE rn <= p_items\n",
    "        ),\n",
    "\n",
    "        -- supplements: rank by tag overlap; keep top p_items\n",
    "        s_ranked AS (\n",
    "          SELECT\n",
    "            c.user_id, c.phase, c.life_stage,\n",
    "            'supplement' AS section,\n",
    "            sc.supplement_id AS id, sc.name,\n",
    "            TO_JSON(NAMED_STRUCT('tags', sc.tags)) AS meta,\n",
    "            ROW_NUMBER() OVER (\n",
    "              ORDER BY SIZE(ARRAY_INTERSECT(sc.tags, r.prefer_supp_tags)) DESC, sc.name ASC\n",
    "            ) AS rn\n",
    "          FROM ctx c\n",
    "          JOIN rules r ON r.phase = c.phase AND r.life_stage = c.life_stage\n",
    "          JOIN {CATALOG}.{SCHEMA}.silver_supplement_catalog sc\n",
    "          WHERE (SIZE(ARRAY_INTERSECT(sc.tags, r.prefer_supp_tags)) > 0 OR SIZE(r.prefer_supp_tags) = 0)\n",
    "            AND ( (c.caffeine_free = TRUE AND ARRAY_CONTAINS(sc.tags, 'caffeine-free')) OR c.caffeine_free = FALSE )\n",
    "        ),\n",
    "        s AS (\n",
    "          SELECT user_id, phase, life_stage, section, id, name, meta\n",
    "          FROM s_ranked\n",
    "          WHERE rn <= p_items\n",
    "        ),\n",
    "\n",
    "        -- workout: pick one by rule (keep rn <= 1)\n",
    "        w_ranked AS (\n",
    "          SELECT\n",
    "            c.user_id, c.phase, c.life_stage,\n",
    "            'workout' AS section,\n",
    "            wc.workout_id AS id, wc.name,\n",
    "            TO_JSON(NAMED_STRUCT('intensity', wc.intensity, 'focus', wc.focus, 'tags', wc.tags)) AS meta,\n",
    "            ROW_NUMBER() OVER (ORDER BY wc.name ASC) AS rn\n",
    "          FROM ctx c\n",
    "          JOIN rules r ON r.phase = c.phase AND r.life_stage = c.life_stage\n",
    "          JOIN {CATALOG}.{SCHEMA}.silver_workout_catalog wc\n",
    "          WHERE wc.intensity = r.workout_intensity\n",
    "        ),\n",
    "        w AS (\n",
    "          SELECT user_id, phase, life_stage, section, id, name, meta\n",
    "          FROM w_ranked\n",
    "          WHERE rn <= 1\n",
    "        ),\n",
    "\n",
    "        -- hygiene: rank matches; keep top p_items\n",
    "        h_ranked AS (\n",
    "          SELECT\n",
    "            c.user_id, c.phase, c.life_stage,\n",
    "            'hygiene' AS section,\n",
    "            hc.hygiene_id AS id, hc.name,\n",
    "            TO_JSON(NAMED_STRUCT('type', hc.type, 'tags', hc.tags)) AS meta,\n",
    "            ROW_NUMBER() OVER (ORDER BY hc.name ASC) AS rn\n",
    "          FROM ctx c\n",
    "          JOIN rules r ON r.phase = c.phase AND r.life_stage = c.life_stage\n",
    "          JOIN {CATALOG}.{SCHEMA}.silver_hygiene_catalog hc\n",
    "          WHERE ARRAY_CONTAINS(r.hygiene_types, hc.type)\n",
    "            AND ( (c.sensitive_skin = TRUE AND ARRAY_CONTAINS(hc.tags, 'sensitive-skin')) OR c.sensitive_skin = FALSE )\n",
    "        ),\n",
    "        h AS (\n",
    "          SELECT user_id, phase, life_stage, section, id, name, meta\n",
    "          FROM h_ranked\n",
    "          WHERE rn <= p_items\n",
    "        )\n",
    "\n",
    "        SELECT * FROM m\n",
    "        UNION ALL SELECT * FROM s\n",
    "        UNION ALL SELECT * FROM w\n",
    "        UNION ALL SELECT * FROM h\n",
    "    \"\"\")\n",
    "    print(\"recommend_wellness_bundle  : function created (no parameterized LIMIT)\")\n",
    "\n",
    "\n",
    "# Run all GOLD steps\n",
    "if RUN_LAYER in (\"ALL\", \"GOLD\"):\n",
    "    gold_cycles()\n",
    "    gold_user_cycle_metrics()\n",
    "    gold_irregularities_view()\n",
    "\n",
    "    gold_user_lifestage_view()\n",
    "    gold_user_current_phase_view()\n",
    "    gold_user_wellness_profile_view()\n",
    "    gold_user_wellness_context_view()\n",
    "\n",
    "    # Optional: create the recommender function as a tool for your agent\n",
    "    create_recommend_wellness_bundle_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c1ae9a-4eb6-4758-a936-8ad31925858a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quick sanity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c5deaf-4eda-48ae-9221-0d750bfa8bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(spark.sql(f\"SELECT * FROM wwc2025.period_pipeline.gold_user_wellness_context   LIMIT 20\"))\n",
    "display(spark.sql(f\"SELECT * FROM wwc2025.period_pipeline.gold_user_current_phase  ORDER BY user_id LIMIT 20\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5739219013701094,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1.raw_data_transform",
   "widgets": {
    "CATALOG": {
     "currentValue": "wwc2025",
     "nuid": "694720aa-68aa-43c1-a15a-826b838052d9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "wwc2025",
      "label": null,
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "wwc2025",
      "label": null,
      "name": "CATALOG",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "RAW_VOLUME": {
     "currentValue": "raw_period",
     "nuid": "91701cf3-b455-442d-ba07-286b52f32c37",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "raw_period",
      "label": null,
      "name": "RAW_VOLUME",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "raw_period",
      "label": null,
      "name": "RAW_VOLUME",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "RUN_LAYER": {
     "currentValue": "ALL",
     "nuid": "d7dd0c9d-b7a6-49ff-ba5b-0c02b74f6e39",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ALL",
      "label": null,
      "name": "RUN_LAYER",
      "options": {
       "choices": [
        "ALL",
        "RAW",
        "BRONZE",
        "SILVER",
        "GOLD"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "ALL",
      "label": null,
      "name": "RUN_LAYER",
      "options": {
       "autoCreated": null,
       "choices": [
        "ALL",
        "RAW",
        "BRONZE",
        "SILVER",
        "GOLD"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "SCHEMA": {
     "currentValue": "period_pipeline",
     "nuid": "602c3565-d965-4e75-b6ca-9ab9a87d7c73",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "period_pipeline",
      "label": null,
      "name": "SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "period_pipeline",
      "label": null,
      "name": "SCHEMA",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
